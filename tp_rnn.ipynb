{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab session is to design and evaluate systems for character-by-character text prediction. These models are capable, given a few input characters, to generate text of arbitrary length.\n",
    "\n",
    "To do so, the study will focus on deep learning models for sequence processing known as recurrent neural networks. We have a dataset representing part of Shakespeare's works, in the form of a plain text file containing about 1.1 million characters (~ 1 Mo). The role of the learning process is thus to correctly predict each character given a sequence of past information.\n",
    "\n",
    "To model the relationship between characters in a text, we will first focus on the construction of a simple recurrent neural network along with its training and testing procedures. A second part will compare the performance of the RNN to that of its variants, the Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) on this text synthesis task.\n",
    "\n",
    "This lab session uses the Python 3 version of the Tensorflow framework, for which an API is available at the adress:\n",
    "https://www.tensorflow.org/api_docs/python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - RNN construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequences are series of elements that, unless random, are structured and often contain regularities. As a result, elements of a sequence can be modeled in relation to others, with invariance properties that the commonly used one-to-one architectures fail to capture.\n",
    "\n",
    "A solution to this problem is the addition of recurrence to this type of models to obtain recurrent neural networks. The RNN introduces a hidden recurrent state that, at each timestep, contains deterministic information from the past sequence elements. Its transformation parameters are shared along time, so that each part of the input sequence is treated identically. Compared to other parameter-sharing models such as CNNs, the RNN has theoretically unlimited memory and can process sequences of varying lengths. A recurrent graph can be unrolled in time as equivalent to a multilayer perceptron, and gradient backpropagation is thus performed using the same method (chain rule).\n",
    "\n",
    "In practice, the implementation of the RNN consists of three different modules similar to other deep learning models:\n",
    " - The definition of the model, including its architecture and parameters, its transformations and its optimization details\n",
    " - The learning process for batches of paired input-output examples\n",
    " - A testing function used for generation, after training\n",
    "  \n",
    "In this part we will implement each of the steps needed to train and test a simple RNN model for character generation. To reduce computational costs only 10% of the available dataset will be used for training, the impact of which will be discussed in part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow, the architecture and the optimization process can be defined in the model in the form of variable containers and operations. During training, they are evaluated as a function of the input and target output. The model definition thus includes several essential parts:\n",
    " - The pre-processing and formatting of inputs and outputs\n",
    " - The initialization of model parameters (weights)\n",
    " - The forward operations apllied to inputs using the previously defined parameters\n",
    " - The definition and computation of a loss function\n",
    " - An optimization procedure to apply parameter updates as a function of backpropagated gradients\n",
    "\n",
    "\n",
    "The skeleton of the code as well as some of the operations are already implemented below. Complete the different parts where indicated using the suggested functions in parenthesis as well as the Tensorflow API:\n",
    " - Placeholders for input and output (tf.Placeholder) are containers which will receive data examples during training and testing.\n",
    " - Input embeddings (tf.get_variable, tf.nn.embedding_lookup) which transform each input character into a vector. In the case of categorical (discrete) input data, embeddings are often preferred as an alternative to one-hot encoding. The embedding process maps each character to a learned real-valued vector of arbitrary dimension. Here we choose rnn_size as the embedding dimension for simplicity.\n",
    " - The definition of the RNN cells (rnn.BasicRNNCell, rnn.MultiRNNCell) with size parameters to initialize weights\n",
    " - The RNN forward process (legacy_seq2seq.rnn_decoder) implements the recurrent transformations given an input sequence\n",
    " - Output softmax probabilities (tf.matmul, tf.nn.softmax) can be computed from the RNN last layer's internal state. A linear transform first maps the RNN output to a real-valued vector, then input to the softmax to obtain a probability value for each character in the vocabulary.\n",
    " - A loss function (legacy_seq2seq.rnn_loss_by_example) compares each output character with its ground truth, the training process aims at maximizing the probability of choosing the right character while minimizing others.\n",
    " - Loss reduction along time and examples (tf.reduce_sum) is a way to compute a single cost value by averaging the error from all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, batch_size, seq_length, vocab_size, training=True):\n",
    "        rnn_size = 128\n",
    "        num_layers = 1\n",
    "        \n",
    "        if not training:\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        \n",
    "        # ----- Inputs-Outputs -----\n",
    "        # Placeholders for input and output data - To complete\n",
    "        self.input_data = ...\n",
    "        self.targets = ...\n",
    "        \n",
    "        # Input embeddings - To complete\n",
    "        embedding = ...\n",
    "        inputs = ...\n",
    "\n",
    "        inputs = tf.split(inputs, seq_length, 1)\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "        \n",
    "        \n",
    "        # ----- Model parameters definition -----\n",
    "        # Definition of the recurrent cells - To complete\n",
    "        cells = []\n",
    "        for _ in range(num_layers):\n",
    "            cells.append(...)\n",
    "        self.cell = cell = ...\n",
    "        \n",
    "        # RNN state to output transformation\n",
    "        with tf.variable_scope('rnnlm'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\",\n",
    "                                        [rnn_size, vocab_size])\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "\n",
    "        \n",
    "        # ----- Model forward definition -----\n",
    "        # Initial state\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # Loop used during generation (test) only\n",
    "        def loop(prev, _):\n",
    "            prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "            return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        # RNN implementation - To complete\n",
    "        outputs, last_state = ...\n",
    "        \n",
    "        self.final_state = last_state\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, rnn_size])\n",
    "\n",
    "        # Output softmax probabilities - To complete\n",
    "        self.logits = ...\n",
    "        self.probs = ...\n",
    "        \n",
    "        # ----- Loss computation -----\n",
    "        # Loss function - To complete\n",
    "        loss = ...\n",
    "        # Loss reduction - To complete\n",
    "        with tf.name_scope('cost'):\n",
    "            self.cost = ...\n",
    "        \n",
    "        # ---- Optimization -----\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5.)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The '):\n",
    "        # Initialize the internal state at the first timestep\n",
    "        state = sess.run(self.cell.zero_state(1, tf.float32))\n",
    "        # Process the prime sequence\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state: state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        # Sampling in a softmax distribution\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        # Character-by-Character generation\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state: state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "            sample = weighted_pick(p)\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of the training process is to optimize the previously implemented model on a given dataset. As such, it operates on a specific instance of the model's class with set dimensions. To facilitate the segmentation and rotation of training data a TextLoader class is implemented in the Python script \"utils.py\".\n",
    "\n",
    "Implement where indicated the instantiation of the TextLoader and Model classes using the proposed meta-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import cPickle\n",
    "from utils import TextLoader\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(100)\n",
    "\n",
    "data_dir = 'data/tinyshakespeare_10'\n",
    "save_dir = 'save'\n",
    "log_dir = 'logs'\n",
    "batch_size = 32\n",
    "seq_length = 50\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the data loader - To complete\n",
    "data_loader = ...\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "# Initialize the RNN model - To complete\n",
    "model = ...\n",
    "\n",
    "# Save dictionary and parameters\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "with open(os.path.join(save_dir, 'config.pkl'), 'wb') as f:\n",
    "    cPickle.dump((batch_size, seq_length, vocab_size), f)\n",
    "with open(os.path.join(save_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "    cPickle.dump((data_loader.chars, data_loader.vocab), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is initialized, the training process can be implemented. In practice, training is performed inside a Tensorflow session that can be run to evaluate the outputs and optimization-related quantities given an input sequence and its ground truth output. At each epoch (iteration over the whole dataset), each batch of input-output examples are passed through the network and backpropagation is used to apply small changes to the model's parameters.\n",
    "\n",
    "As the different processing steps are implemented in the model, the training implementation only needs to cover data management and run the model. Complete the code below using the suggested functions:\n",
    "\n",
    " - Set the model's learning rate to 0.002 (sess.run, tf.assign)\n",
    " - Reset the batch pointer at each new epoch, corresponding to an iteration over the whole dataset (see utils.py)\n",
    " - Load the current batch of data examples (see utils.py and Tensorflow placeholders)\n",
    " - Run the model to perform an optimization step on the current batch, and return the loss (sess.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    # Set the model learning rate to 0.002 - To complete\n",
    "    sess.run(tf.assign(model.lr, 0.002))\n",
    "    \n",
    "    losses = []\n",
    "    for e in range(num_epochs):\n",
    "        # Reset the batch pointer - To complete\n",
    "        ...\n",
    "        \n",
    "        # Compute the initial state\n",
    "        state = sess.run(model.initial_state)\n",
    "        for b in range(data_loader.num_batches):\n",
    "            # Load the current batch of data - To complete\n",
    "            ...\n",
    "            \n",
    "            # Run the model for the current batch - To complete\n",
    "            ...\n",
    "            \n",
    "            losses.append(train_loss)\n",
    "            print(\"{}/{} (epoch {}), train_loss = {:.3f}\"\n",
    "                  .format(e * data_loader.num_batches + b,\n",
    "                          num_epochs * data_loader.num_batches,\n",
    "                          e, train_loss))\n",
    "    # Save the trained model\n",
    "    checkpoint_path = os.path.join(save_dir, 'model.ckpt')\n",
    "    saver.save(sess, checkpoint_path,\n",
    "               global_step=e * data_loader.num_batches + b)\n",
    "    print(\"model saved to {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss at each batch is saved in the losses list. Display the corresponding loss curve using Matplotlib and comment on its evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained and saved, we can use it to generate text sequences of arbitrary length. This is implemented in the sample() function, which takes two arguments:\n",
    " - The length of the sequence to generate\n",
    " - The prime sequence that we want to build a text upon\n",
    "\n",
    "This function must first construct the model and load the learnt parameters before generating and saving the desired text sequence. Complete the definition of the function with:\n",
    " - The model initialization with training mode disabled\n",
    " - The generation procedure for the defined parameters\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def sample(n = 1000, prime = u' '):\n",
    "    tf.reset_default_graph()\n",
    "    save_dir = 'save'\n",
    "    sample = 1\n",
    "\n",
    "    tf.set_random_seed(100)\n",
    "    \n",
    "    # Load parameters and dictionary\n",
    "    with open(os.path.join(save_dir, 'config.pkl'), 'rb') as f:\n",
    "        batch_size, seq_length, vocab_size = cPickle.load(f)\n",
    "    with open(os.path.join(save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "        chars, vocab = cPickle.load(f)\n",
    "        \n",
    "    # Initialize the model - To complete\n",
    "    ...\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # Load the model\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            # Sample - To complete\n",
    "            ...\n",
    "            \n",
    "            # Write the generated text to a file and to the console\n",
    "            with codecs.open(os.path.join(save_dir, \"sample.txt\"), \"w\", \"utf-8-sig\") as temp:\n",
    "                temp.write(sample_test)\n",
    "\n",
    "            print(sample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the sample() function using a large sequence length (eg: 1000). Describe qualitatively the performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Comparison of recurrent models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple recurrent neural networks suffer from problems related to the transmission of long-term information. During backpropagation, the gradient is multiplied by the recurrent weight matrix for each timestep. Depending on the spectral radius (i.e. largest singular value) of this matrix, the gradient can exponentially explode or vanish making the learning of long-term dependancies difficult. As these parameters are learned, there is no way to ensure that gradient vanishing will not impair the performance of the network.\n",
    "Variants of the basic RNN cell have been proposed that ensure a good flow of information along time via the \"constant error flow\" principle: the long short-term memory (LSTM) and gated recurrent units (GRU). These models tend to better capture long-term structures in sequences at the cost of increased computational and memory complexities.\n",
    "\n",
    "Additionally, the performance of deep learning models heavily depends on the dimensions of the architectures, which is chosen semi-arbitrarily, as well as on both the quantity and quality of the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Qualitative evaluation of generated sample quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several models have been trained to compare the quality of generated text based on a set of factors:\n",
    " - Recurrent cell types (RNN, LSTM, GRU)\n",
    " - Dataset size (10p - 100k characters, 1M characters)\n",
    " - Internal state dimensions (128, 512)\n",
    " - Number of recurrent layers (1, 3)\n",
    "\n",
    "The corresponding models are saved in folders using the following name structure: \n",
    "\n",
    "save_[RNN type]_ [# layers]_ [Internal state size]_ [Training epochs]_ [Dataset size]\n",
    "\n",
    "Samples can be obtained using the sample() function defined in sample.py (example below).\n",
    "\n",
    "Generate long text sequences for each trained model and qualitatively compare the results as a function of the experimental factors described above. Examples are saved in a \"sample.txt\" file in the model folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sampling.sample import sample as sample2\n",
    "sample2('save_rnn_1_128_10e_10p', 2000)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Quantitative evaluation of generated sample quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only objective quantitative metric available to evaluate the performance of generative models is the cost function, which the learning process tries to minimize. However, it is often insufficient when modeling complex structures such as written language, for which several implicit rules must be observed (grammar, sentence structure...).\n",
    "\n",
    " - Propose a method to evaluate the quality of generated extracts, in the form of a quantitative indicator (0-1 scale). Implement it and apply it to generated sequences from the trained models. For this part you can use an external Python script or Jupyter Notebook.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative indicator implementation\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
